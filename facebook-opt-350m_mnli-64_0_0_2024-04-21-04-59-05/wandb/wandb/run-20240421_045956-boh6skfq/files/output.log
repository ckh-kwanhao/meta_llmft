Trainable parameters: ['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.project_out.weight', 'model.decoder.project_in.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'score.dense.weight', 'score.dense.bias', 'score.output.weight', 'score.output.bias']
Using /cache/torch as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /cache/torch/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module fused_adam...
Time to load fused_adam op: 0.18332862854003906 seconds
Using /cache/torch as PyTorch extensions root...
Emitting ninja build file /cache/torch/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Time to load utils op: 0.1682429313659668 seconds
Parameter Offload: Total persistent parameters: 321026 in 243 params
  0%|                                                                                 | 0/2048 [00:00<?, ?it/s]
Using /cache/torch as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
  0%|                                                                       | 1/2048 [00:03<1:45:44,  3.10s/it][WARNING|trainer_pt_utils.py:849] 2024-04-21 05:00:05,231 >> tried to get lr value before scheduler/optimizer started stepping, returning lr=0
  0%|                                                                       | 1/2048 [00:03<1:45:44,  3.10s/it]





















  3%|██▎                                                                     | 64/2048 [00:46<21:48,  1.52it/s]
  1%|▌                                                                         | 4/500 [00:01<02:03,  4.01it/s]















































  0%|▎                                                                         | 2/500 [00:00<00:48, 10.33it/s]















































  1%|▊                                                                         | 7/670 [00:01<01:49,  6.03it/s]
































































 99%|███████████████████████████████████████████████████████████████████████▍| 665/670 [02:07<00:01,  3.59it/s]





















  6%|████▍                                                                  | 128/2048 [06:57<21:06,  1.52it/s]
  0%|▎                                                                         | 2/500 [00:00<01:14,  6.69it/s]
















































 98%|██████████████████████████████████████████████████████████████████████▌ | 490/500 [01:35<00:01,  5.07it/s]
  2%|█▌                                                                       | 11/500 [00:01<01:30,  5.41it/s]
















































  1%|▍                                                                         | 4/670 [00:00<01:38,  6.77it/s]

































































 99%|███████████████████████████████████████████████████████████████████████▏| 662/670 [02:08<00:01,  5.30it/s]






















  9%|██████▋                                                                | 192/2048 [13:06<21:18,  1.45it/s]
{'loss': 0.3971, 'learning_rate': 1.8066406250000002e-05, 'total_grad_norm': 0.003063806128147059, 'epoch': 3.0}















































  0%|▎                                                                         | 2/500 [00:00<00:43, 11.44it/s]














































 98%|██████████████████████████████████████████████████████████████████████▍ | 489/500 [01:32<00:01,  5.57it/s]
































































100%|███████████████████████████████████████████████████████████████████████▋| 667/670 [02:06<00:00,  5.53it/s]






















 12%|████████▊                                                              | 255/2048 [19:18<19:37,  1.52it/s]
 12%|████████▉                                                              | 256/2048 [19:18<19:39,  1.52it/s]














































100%|███████████████████████████████████████████████████████████████████████▊| 499/500 [01:33<00:00,  5.47it/s]
















































 99%|███████████████████████████████████████████████████████████████████████▌| 497/500 [01:35<00:00,  5.48it/s]































































 99%|███████████████████████████████████████████████████████████████████████▏| 662/670 [02:05<00:01,  5.34it/s]






















 15%|██████████▉                                                            | 317/2048 [25:28<18:54,  1.53it/s]
 16%|███████████                                                            | 320/2048 [25:30<18:51,  1.53it/s]















































 98%|██████████████████████████████████████████████████████████████████████▊ | 492/500 [01:34<00:01,  4.69it/s]
















































100%|███████████████████████████████████████████████████████████████████████▋| 498/500 [01:34<00:00,  5.42it/s]































































 99%|███████████████████████████████████████████████████████████████████████▏| 662/670 [02:05<00:01,  5.38it/s]






















 19%|█████████████▎                                                         | 383/2048 [31:31<17:57,  1.55it/s]
 19%|█████████████▎                                                         | 384/2048 [31:32<18:00,  1.54it/s]















































 99%|███████████████████████████████████████████████████████████████████████▍| 496/500 [01:35<00:00,  4.89it/s]















































100%|███████████████████████████████████████████████████████████████████████▋| 498/500 [01:32<00:00,  5.53it/s]





























































 99%|███████████████████████████████████████████████████████████████████████▏| 662/670 [02:01<00:01,  5.62it/s]





















 22%|███████████████▍                                                       | 447/2048 [37:36<16:39,  1.60it/s]
 22%|███████████████▌                                                       | 448/2048 [37:36<16:40,  1.60it/s]












































100%|███████████████████████████████████████████████████████████████████████▋| 498/500 [01:29<00:00,  5.86it/s]












































 99%|███████████████████████████████████████████████████████████████████████▏| 494/500 [01:27<00:01,  5.73it/s]




























































 99%|███████████████████████████████████████████████████████████████████████▌| 666/670 [01:59<00:00,  5.70it/s]





















 25%|█████████████████▊                                                     | 512/2048 [43:26<16:57,  1.51it/s]
{'loss': 0.0, 'learning_rate': 4.921875e-05, 'total_grad_norm': 0.006395815354658084, 'epoch': 8.0}











































  0%|                                                                                  | 0/500 [00:00<?, ?it/s]













































 99%|███████████████████████████████████████████████████████████████████████▍| 496/500 [01:29<00:00,  5.64it/s]




























































 99%|███████████████████████████████████████████████████████████████████████▌| 666/670 [01:59<00:00,  5.73it/s]




















 28%|███████████████████▉                                                   | 574/2048 [49:05<16:33,  1.48it/s]
 28%|███████████████████▉                                                   | 576/2048 [49:06<15:45,  1.56it/s]











































 99%|███████████████████████████████████████████████████████████████████████▏| 494/500 [01:26<00:01,  5.83it/s]













































100%|███████████████████████████████████████████████████████████████████████▊| 499/500 [01:28<00:00,  4.47it/s]


























































100%|███████████████████████████████████████████████████████████████████████▋| 667/670 [01:55<00:00,  4.51it/s]




















 31%|██████████████████████                                                 | 638/2048 [54:49<14:32,  1.62it/s]
 31%|██████████████████████▏                                                | 640/2048 [54:50<15:07,  1.55it/s]












































 99%|███████████████████████████████████████████████████████████████████████▌| 497/500 [01:29<00:00,  5.34it/s]












































100%|███████████████████████████████████████████████████████████████████████▋| 498/500 [01:27<00:00,  5.82it/s]




























































 99%|███████████████████████████████████████████████████████████████████████▎| 664/670 [01:57<00:01,  5.92it/s]





















 34%|███████████████████████▋                                             | 704/2048 [1:00:38<30:39,  1.37s/it]
{'loss': 2.1921, 'learning_rate': 6.767578125e-05, 'total_grad_norm': 8.72784094514216, 'epoch': 11.0}












































  0%|▎                                                                         | 2/500 [00:00<00:42, 11.76it/s]












































  1%|▍                                                                         | 4/670 [00:00<01:30,  7.33it/s]




























































 99%|███████████████████████████████████████████████████████████████████████▎| 664/670 [01:58<00:00,  6.01it/s]



















 38%|█████████████████████████▉                                           | 768/2048 [1:06:16<13:14,  1.61it/s]
  0%|▎                                                                         | 2/500 [00:00<00:42, 11.70it/s]












































100%|███████████████████████████████████████████████████████████████████████▊| 499/500 [01:28<00:00,  5.91it/s]













































  0%|▏                                                                         | 2/670 [00:00<00:56, 11.77it/s]



























































100%|███████████████████████████████████████████████████████████████████████▉| 669/670 [01:58<00:00,  5.58it/s]





















 41%|████████████████████████████                                         | 832/2048 [1:12:04<14:57,  1.35it/s]
  1%|▋                                                                         | 5/500 [00:00<01:17,  6.42it/s]












































  1%|▋                                                                         | 5/500 [00:00<01:15,  6.57it/s]













































100%|███████████████████████████████████████████████████████████████████████▊| 499/500 [01:31<00:00,  5.22it/s]






























































 99%|███████████████████████████████████████████████████████████████████████▏| 662/670 [02:01<00:01,  5.06it/s]



















 44%|██████████████████████████████▏                                      | 896/2048 [1:17:47<11:40,  1.64it/s]
  1%|▌                                                                         | 4/500 [00:00<01:06,  7.40it/s]













































 99%|███████████████████████████████████████████████████████████████████████▌| 497/500 [01:30<00:00,  5.81it/s]













































  1%|▋                                                                         | 6/670 [00:00<01:45,  6.32it/s]




























































 99%|███████████████████████████████████████████████████████████████████████ | 661/670 [01:58<00:01,  5.70it/s]




















 47%|████████████████████████████████▎                                    | 960/2048 [1:23:38<11:21,  1.60it/s]
{'loss': 2.0534, 'learning_rate': 9.267578125e-05, 'total_grad_norm': 0.7461730251297759, 'epoch': 15.0}












































  0%|▎                                                                         | 2/500 [00:00<00:44, 11.08it/s]


